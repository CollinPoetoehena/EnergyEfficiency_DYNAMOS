{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "tags": []
            },
            "source": [
                "# Setup and Configure DYNAMOS in the created Kubernetes environment in FABRIC \n",
                "\n",
                "This Jupyter notebook will configure the Kubernetes environment so that DYNAMOS can run in it.\n",
                "\n",
                "FABRIC API docs: https://fabric-fablib.readthedocs.io/en/latest/index.html\n",
                "\n",
                "\n",
                "## Step 1: Configure the Environment Configure the Environment (has to be done once in the Jupyter Hub environment), Create Slice & Setup K8s Cluster\n",
                "\n",
                "Before running this notebook, you will need to configure your environment using the [Configure Environment](../configure_and_validate.ipynb) notebook. Please stop here, open and run that notebook, then return to this notebook. Note: this has to be done only once in the Jupyter Hub environment (unless configuration is removed/deleted of course).\n",
                "\n",
                "If you are using the FABRIC JupyterHub many of the environment variables will be automatically configured for you.  You will still need to set your bastion username, upload your bastion private key, and set the path to where you put your bastion private key. Your bastion username and private key should already be in your possession.  \n",
                "\n",
                "After following all steps of the Configuring Environment notebook, you should be able to run this notebook without additional steps.\n",
                "\n",
                "Next, you will need to have setup the slice in FABRIC using the [Create Slice](../create_slice.ipynb) notebook.\n",
                "\n",
                "Finally, you will need to prepare the Kubernetes environment in FABRIC using the [Configure Kubernetes](../k8s-cluster-setup/k8s_setup.ipynb) notebook.\n",
                "\n",
                "More information about accessing your experiments through the FABRIC bastion hosts can be found [here](https://learn.fabric-testbed.net/knowledge-base/logging-into-fabric-vms/).\n",
                " "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "tags": []
            },
            "source": [
                "## Step 2: Setup the Environment for this Notebook\n",
                "\n",
                "### Step 2.1: Import FABRIC API and other libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "import json\n",
                "import traceback\n",
                "import time\n",
                "\n",
                "from fabrictestbed_extensions.fablib.fablib import FablibManager as fablib_manager\n",
                "\n",
                "fablib = fablib_manager()\n",
                "\n",
                "fablib.show_config();\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Step 2.2: Configure the parameters and variables\n",
                "Can be used to set the corresponding slice and other variables used for subsequent cells."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "slice_name = 'DYNAMOS_EnergyEfficiency'\n",
                "# Nodes:\n",
                "node1_name = 'k8s-control-plane'\n",
                "node2_name = 'dynamos-core'\n",
                "node3_name = 'vu'\n",
                "node4_name = 'uva'\n",
                "node5_name = 'surf'"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 3: Manual steps to prepare the node\n",
                "This step explains the manual steps that are required before running the automated steps in the next steps.\n",
                "\n",
                "SSH commands to go into the VMs terminal can be fetched from the previous notebook: [Configure Kubernetes](../k8s-cluster-setup/k8s_setup.ipynb).\n",
                "\n",
                "### Manual steps to execute:\n",
                "```sh\n",
                "# SSH into the kubernetes control-plane node.\n",
                "# Create DYNAMOS folder on the control-plane node for the DYNAMOS application:\n",
                "mkdir -p DYNAMOS\n",
                "\n",
                "# Then from your local machine in a Linux terminal, such as WSL, run:\n",
                "# Go to the correct directory in this project, such as in a VSC terminal in WSL:\n",
                "cd fabric/scripts\n",
                "# Upload the configuration folder for DYNAMOS to the kubernetes control-plane node, such as (replace IP of course in the ssh_config file below if necessary):\n",
                "./upload_to_remote.sh ../../configuration ~/.ssh/slice_key ../fabric_config/ssh_config_upload_script ubuntu dynamos-node \"~/DYNAMOS\"\n",
                "# Upload the charts folder in the DYNAMOS folder, such as (replace IP of course in the ssh_config file below if necessary):\n",
                "./upload_to_remote.sh ../dynamos/charts ~/.ssh/slice_key ../fabric_config/ssh_config_upload_script ubuntu dynamos-node \"~/DYNAMOS\"\n",
                "# Replace the congiguration script in this folder with the FABRIC specific configuration script, such as (replace IP of course in the ssh_config file below if necessary):\n",
                "./upload_to_remote.sh ../dynamos/dynamos-configuration.sh ~/.ssh/slice_key ../fabric_config/ssh_config_upload_script ubuntu dynamos-node \"~/DYNAMOS/configuration\"\n",
                "# Optionally add the script for quickly uninstalling for development purposes\n",
                "./upload_to_remote.sh ../dynamos/uninstall-dynamos-configuration.sh ~/.ssh/slice_key ../fabric_config/ssh_config_upload_script ubuntu dynamos-node \"~/DYNAMOS/configuration\"\n",
                "\n",
                "# IMPORTANT: when changing one of the above files/folders, you have to reupload it to the node before executing!\n",
                "\n",
                "# Workaround for no internet access for etcd init job: manually add the files in the location, see etcd-pvc.yaml:\n",
                "# SSH into dynamos-core node and create directory:\n",
                "mkdir -p DYNAMOS\n",
                "# Upload the configuration folder to the dynamos-core node (after changing the IP in fabric/fabric_config/ssh_config_upload_script temporarily to dynamos-core IP):\n",
                "./upload_to_remote.sh ../../configuration ~/.ssh/slice_key ../fabric_config/ssh_config_upload_script ubuntu dynamos-node \"~/DYNAMOS\"\n",
                "# Then in SSH again, run the following:\n",
                "sudo mkdir -p /mnt/etcd-data\n",
                "# Then copy the files to the location of the persistent volume:\n",
                "sudo cp ~/DYNAMOS/configuration/etcd_launch_files/*.json /mnt/etcd-data\n",
                "sudo chmod -R 777 /mnt/etcd-data\n",
                "# Verify files:\n",
                "ls /mnt/etcd-data\n",
                "# Then in the ssh_config_upload_script, change back to the IP of the k8s-control-plane node.\n",
                "# After executing the dynamos-configuration.sh script, you can view the logs of the init-etcd-pvc pod in k9s for example, where you should see something like this for each file:\n",
                "-rwxrwxrwx    1 root     root          1746 Apr 15 12:00 agreements.json\n",
                "\n",
                "# SSH into the k8s-control-plane node again.\n",
                "# Make sure docker is logged in after:\n",
                "docker login -u poetoec\n",
                "# Enter the password with a PAT, see https://app.docker.com/settings\n",
                "# Go to the DYNAMOS/configuration directory\n",
                "cd DYNAMOS/configuration\n",
                "\n",
                "# (Note: for the next steps, you can quickly uninstall using the uninstall-dynamos-configuration.sh script after ./dynamos-configuration.sh):\n",
                "./uninstall-dynamos-configuration.sh\n",
                "# Now you can move on to the next steps\n",
                "```\n",
                "Note: this uses a different ssh_config file specific for the nodes, otherwise, it encountered an error such as \"ssh: Could not resolve hostname bastion.fabric-testbed.net: Temporary failure in name resolution\". Do not forget to change the IP when new nodes are created in FABRIC.\n",
                "\n",
                "Note: when making changes, the changed files need to be uploaded to the VM again before executing them, such as the dynamos-configuration.sh script and the charts folder.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 4: Installing and configuring DYNAMOS\n",
                "This step installs and configures DYNAMOS in the FABRIC Kubernetes environment. See the README.md in this folder for any additional information.\n",
                "\n",
                "Make sure you followed every important step of each notebook, such as the IPv4 acccess, which will otherwise cause some things to fail, such as the Linkerd setup (see Troubleshooting.md).\n",
                "\n",
                "Note: This guide for monitoring is used from energy-efficiency/docs/getting-started/3_SetupMonitoring.md, but tailored for the FABRIC setup. Therefore, this guide here will only explain how to execute the script and some specifics for FABRIC, the .md file in the above location gives more information.\n",
                "\n",
                "Note: kubernetes dashboard is skipped here, since it is not necessary and is not as easy to access because Kubernetes is not running on localhost. Therefore, k9s can be used for this completely."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "try:\n",
                "    # Get slice by name: https://fabric-fablib.readthedocs.io/en/latest/fablib.html#fabrictestbed_extensions.fablib.fablib.FablibManager.get_slice\n",
                "    slice = fablib.get_slice(name=slice_name)\n",
                "    # Get the control plane node (node1)\n",
                "    node1 = slice.get_node(name=node1_name)\n",
                "    \n",
                "    # Upload script files to the node (cannot do ~, this will cause No such file, need to do relative path from home of the node):\n",
                "    file_attributes = node1.upload_file(local_file_path=\"linkerd.sh\", remote_file_path=\"./DYNAMOS/configuration/linkerd.sh\")\n",
                "    file_attributes = node1.upload_file(local_file_path=\"dynamos-configuration.sh\", remote_file_path=\"./DYNAMOS/configuration/dynamos-configuration.sh\")\n",
                "    file_attributes = node1.upload_file(local_file_path=\"monitoring.sh\", remote_file_path=\"./DYNAMOS/configuration/monitoring.sh\")\n",
                "\n",
                "    # Add necessary permissions and execute the scripts:\n",
                "    # Setup linkerd\n",
                "    stdout, stderr = node1.execute(f\"chmod +x ./DYNAMOS/configuration/linkerd.sh && ./DYNAMOS/configuration/linkerd.sh\")\n",
                "    # Wait shortly in between scripts to make sure everything is stable again\n",
                "    print(\"Waiting shortly for everything to stabilize...\")\n",
                "    time.sleep(10)\n",
                "    # Install/configure DYNAMOS\n",
                "    stdout, stderr = node1.execute(f\"chmod +x ./DYNAMOS/configuration/dynamos-configuration.sh && ./DYNAMOS/configuration/dynamos-configuration.sh\")\n",
                "    # Wait shortly in between scripts to make sure everything is stable again\n",
                "    print(\"Waiting shortly for everything to stabilize...\")\n",
                "    time.sleep(10)\n",
                "    # Setup monitoring\n",
                "    stdout, stderr = node1.execute(f\"chmod +x ./DYNAMOS/configuration/monitoring.sh && ./DYNAMOS/configuration/monitoring.sh\")\n",
                "    # Wait shortly in between scripts to make sure everything is stable again\n",
                "    print(\"Waiting shortly for everything to stabilize...\")\n",
                "    time.sleep(10)\n",
                "\n",
                "    # Upload the helper script that can be loaded into the SSH session to execute commands for DYNAMOS:\n",
                "    file_attributes = node1.upload_file(local_file_path=\"dynamos-configs.sh\", remote_file_path=\"./DYNAMOS/configuration/dynamos-configs.sh\")\n",
                "    # Note: to load this script in the SSH session on the k8s-control plane node, run:\n",
                "    # cd DYNAMOS/configuration\n",
                "    # source ./dynamos-configs.sh\n",
                "    # Now you can run functions, such as: deploy_agents\n",
                "\n",
                "except Exception as e:\n",
                "    print(f\"Fail: {e}\")\n",
                "    traceback.print_exc()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Step 4.1: Manually patch monitoring to use the UIs on your local machine\n",
                "This step explains how to setup monitoring for Prometheus and Grafana to allow access to their UIs from your local machine (this also needs to be done once only in the kubernetes cluster):\n",
                "```sh\n",
                "# First we tried an SSH tunnel with port-forward, but that caused a lot of issues and did not work, such as installing socat, but still it kept giving \"socat not found\", etc. So, instead of relying on kubectl port-forward, which proxies traffic via the Kubernetes API server and depends on internal tooling (socat), we instead: Exposed Prometheus using a NodePort.\n",
                "# This is a safe change, it will not break Prometheus or Grafana — as long as the change is only to the Service type, and the underlying pods are running normally. It only adds a NodePort to allow external access outside the kubernetes cluster without affecting the pods of the service, in this case Prometheus.\n",
                "\n",
                "# So, first SSH into the k8s-control-plane node in a terminal session, like explained in the fabric/k8s/k8s_setup.ipynb notebook and execute the following commands.\n",
                "# This tells Kubernetes to map Prometheus's internal port (9090) to a static port on the control-plane node itself (e.g., 30906). This allows external access via that high-numbered port — as long as you can reach the node:\n",
                "kubectl patch svc prometheus-kube-prometheus-prometheus -n monitoring -p '{\"spec\": {\"type\": \"NodePort\"}}'\n",
                "# Do the same for Grafana (and any other services you want this for):\n",
                "kubectl patch svc prometheus-grafana -n monitoring -p '{\"spec\": {\"type\": \"NodePort\"}}'\n",
                "```\n",
                "Later steps will verify the setup for this and test the monitoring setup.\n",
                "\n",
                "### Step 4.2: Load utility functions for further usage of DYNAMOS\n",
                "This step explains the final step to deploy the utility functions from DYNAMOS on the k8s-control-plane node, so that the kubernetes environment can be easily managed with these functions, etc.\n",
                "\n",
                "You can change this file whenever you want, such as adding or removing helpful functions. After changes you have to load it in the shell again each time. Also, for each terminal you have to load the file, so it is recommended to use one terminal to execute those functions when developing.\n",
                "\n",
                "Note: to load this script in the SSH session on the k8s-control plane node after adding it with the below Python code, run:\n",
                "```sh\n",
                "# SSH into the k8s-control-plane node and go to the corresponding location:\n",
                "cd DYNAMOS/configuration\n",
                "# Load the script in the terminal session:\n",
                "source ./dynamos-configs.sh\n",
                "# Now you can run functions, such as\n",
                "deploy_agents\n",
                "# You need to load the file in the shell each time you restart a shell or when making changing to the dynamos-configs.sh script\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "try:\n",
                "    # Get slice by name: https://fabric-fablib.readthedocs.io/en/latest/fablib.html#fabrictestbed_extensions.fablib.fablib.FablibManager.get_slice\n",
                "    slice = fablib.get_slice(name=slice_name)\n",
                "    # Get the control plane node (node1)\n",
                "    node1 = slice.get_node(name=node1_name)\n",
                "\n",
                "    # Upload the helper script that can be loaded into the SSH session to execute commands for DYNAMOS:\n",
                "    file_attributes = node1.upload_file(local_file_path=\"dynamos-configs.sh\", remote_file_path=\"./DYNAMOS/configuration/dynamos-configs.sh\")\n",
                "\n",
                "except Exception as e:\n",
                "    print(f\"Fail: {e}\")\n",
                "    traceback.print_exc()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 5: Verify Setup & Test DYNAMOS\n",
                "This step explains how to test if DYNAMOS is working as expected.\n",
                "\n",
                "SSH commands to go into the VMs terminal can be fetched from the previous notebook: [Configure Kubernetes](../k8s-cluster-setup/k8s_setup.ipynb).\n",
                "\n",
                "### Step 5.1: Verify DYNAMOS application\n",
                "To test if DYNAMOS is working correctly, follow the steps below:\n",
                "1. Open k9s in an SSH session on the k8s-control-plane node and verify all important pods are running (some pods can be skipped, see README.md in this folder for more information on some of these remaining issues for pods that can be ignored).\n",
                "2. Follow the below steps to execute some requests to test DYNAMOS:\n",
                "```sh\n",
                "# Test DYNAMOS (without having to add something in the host files, this uses the kubernetes cluster's nodes and setup):\n",
                "# First list the ingress service:\n",
                "kubectl get svc -n ingress -A\n",
                "# Check the api-gateway and nginx-nginx-ingress-controller ones (this one's EXTERNAL-IP is likely still pending, which is fine for FABRIC environment, this is mainly for clouds like AWS). For example:\n",
                "ubuntu@k8s-control-plane:~/DYNAMOS/configuration$ kubectl get svc -n ingress -A\n",
                "NAMESPACE        NAME                             TYPE           CLUSTER-IP       EXTERNAL-IP   PORT(S)                                                          AGE\n",
                "api-gateway      api-gateway                      ClusterIP      10.100.178.113   <none>        8080/TCP                                                         12m\n",
                "...\n",
                "default          kubernetes                       ClusterIP      10.96.0.1        <none>        443/TCP                                                          3h9m\n",
                "ingress          nginx-nginx-ingress-controller   LoadBalancer   10.102.149.144   <pending>     80:31924/TCP,443:31752/TCP                                       12m\n",
                "# Now you can see that the ingress exposes port 80 through 31924, which you should use as the port\n",
                "# Now list the nodes to get the NODE-IP:\n",
                "kubectl get nodes -o wide\n",
                "# Select the dynamos-core node, such as in this case: 10.145.1.6\n",
                "ubuntu@k8s-control-plane:~/DYNAMOS/configuration$ kubectl get nodes -o wide\n",
                "NAME                STATUS   ROLES           AGE     VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION     CONTAINER-RUNTIME\n",
                "dynamos-core        Ready    <none>          3h12m   v1.31.7   10.145.1.6    <none>        Ubuntu 24.04.1 LTS   6.8.0-49-generic   docker://28.0.4\n",
                "...\n",
                "# The ingress host under the rule in charts/api-gateway/templates/api-gateway.yaml is api-gateway.api-gateway.svc.cluster.local, so use that as the host, and the path that can be used in the url for the request is \"/api/v1\" (also from the same yaml file, but then at the path). The url is http, since the ingress specified http.\n",
                "# In the command below the Host is the ingress rule host for the api-gateway. The url is in the format: http://<NODE-IP>:<INGRESS-CONTROLLER-PORT><API-GATEWAY-RULE-PATH>/<ENDPOINT-FROM-API-GATEWAY>. The rest is just the content body that is passed with the request.\n",
                "# Then execute the below command to test DYNAMOS (can be on the k8s-controle-plane node, since this is the most logical/best node to run it from because this is the central node for kubernetes and already runs all other scripts).\n",
                "curl -H \"Host: api-gateway.api-gateway.svc.cluster.local\" \\\n",
                "  http://10.145.1.6:31924/api/v1/requestApproval \\\n",
                "  -H \"Content-Type: application/json\" \\\n",
                "  -d '{\n",
                "    \"type\": \"sqlDataRequest\",\n",
                "    \"user\": {\n",
                "        \"id\": \"12324\",\n",
                "        \"userName\": \"jorrit.stutterheim@cloudnation.nl\"\n",
                "    },\n",
                "    \"dataProviders\": [\"UVA\"]\n",
                "}'\n",
                "# If it gives back something like this then you are good and DYNAMOS is working:\n",
                "{\n",
                "    \"authorized_providers\": {\n",
                "        \"UVA\": \"uva.uva.svc.cluster.local\"\n",
                "    },\n",
                "    \"jobId\": \"jorrit-stutterheim-8e83402f\"\n",
                "}\n",
                "\n",
                "# Another test: data request, such as from the uva (again run it from the k8s-control-plane node, like explained above):\n",
                "# The ingress host under the rule in charts/agents/templates/uva.yaml is uva.uva.svc.cluster.local, so use that as the host, and the path that can be used in the url for the request is \"/agent/v1/sqlDataRequest/uva\" (also from the same yaml file, but then at the path). The url is http, since the ingress specified http.\n",
                "# The rest follows the same format as the curl command above, but now replaced with these specific values, and including the header for Authorization that is required for this request (now just a placeholder in the current DYNAMOS setup).\n",
                "# NOTE: do not forget to replace the job-id with the response from the previous curl command.\n",
                "curl -H \"Host: uva.uva.svc.cluster.local\" \\\n",
                "  http://10.145.1.6:31924/agent/v1/sqlDataRequest/uva \\\n",
                "  -H \"Content-Type: application/json\" \\\n",
                "  -H \"Authorization: bearer 1234\" \\\n",
                "  -d '{\n",
                "    \"type\": \"sqlDataRequest\",\n",
                "    \"query\": \"SELECT DISTINCT p.Unieknr, p.Geslacht, p.Gebdat, s.Aanst_22, s.Functcat, s.Salschal as Salary FROM Personen p JOIN Aanstellingen s ON p.Unieknr = s.Unieknr LIMIT 30000\",\n",
                "    \"algorithm\": \"\",\n",
                "    \"options\": {\n",
                "        \"graph\": false,\n",
                "        \"aggregate\": false\n",
                "    },\n",
                "    \"user\": {\n",
                "        \"id\": \"12324\",\n",
                "        \"userName\": \"jorrit.stutterheim@cloudnation.nl\"\n",
                "    },\n",
                "    \"requestMetadata\": {\n",
                "        \"jobId\": \"jorrit-stutterheim-8e83402f\"\n",
                "    }\n",
                "}'\n",
                "# If it gives back something with data then this is working as well!\n",
                "# Note: this is the same for other agents, such as vu, and the third party, but then with the values of their respective files.\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Step 4.2: Test Monitoring Setup\n",
                "This step tests the monitoring setup after patching the services with NodePort (see previous steps for monitoring). Follow the below steps to verify access to monitoring UIs and to see if everything is working as expected for monitoring:\n",
                "```sh\n",
                "# To check which external port Prometheus was assigned, run:\n",
                "kubectl get svc prometheus-kube-prometheus-prometheus -n monitoring\n",
                "# This can give an output like:\n",
                "# For example if this is the result of getting the service:\n",
                "ubuntu@k8s-control-plane:~$ kubectl get svc prometheus-kube-prometheus-prometheus -n monitoring\n",
                "NAME                                    TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)                         AGE\n",
                "prometheus-kube-prometheus-prometheus   NodePort   10.111.200.101   <none>        9090:30906/TCP,8080:31028/TCP   14h\n",
                "# Here 9090:30906/TCP means:\n",
                "# 9090: the internal port Prometheus listens on inside the Pod/Container, it is defined in the Service spec as the targetPort. This is inside the Kubernetes Pod, not the node itself.\n",
                "# 30906: the external (NodePort) mapped on the node’s public IP\n",
                "\n",
                "# Next, on your local machine, go to the path from where you did the SSH connect commands (described in fabric/k8s/k8s_setup.ipynb notebook). Do not yet SSH into the FABRIC node, this below command will do that for you with an SSH tunnel.\n",
                "# Create an SSH tunnel from your local machine with the NodePort from above, such as (replace with correct IP of the k8s-control-plane node, same as SSH connect command IP):\n",
                "ssh -i ~/.ssh/slice_key -F ssh_config -L 9090:localhost:30906 ubuntu@2001:610:2d0:fabc:f816:3eff:fea4:5b34\n",
                "# This creates an SSH tunnel from your local machine to the FABRIC node’s external IP (the same one you normally SSH into), forwarding local port 9090 (on your local machine, i.e. laptop or workstation) to the NodePort 30906. In short it tells SSH: Please take any request to localhost:9090 on my local machine, send it over the SSH tunnel, and on the remote FABRIC node, connect it to localhost:30906 (which is the NodePort of the FABRIC node Prometheus is exposed on).\n",
                "# Access it at (on your local machine): http://localhost:9090/\n",
                "# -L 9090:localhost:30906 means: [LOCAL_PORT]:[REMOTE_HOST]:[REMOTE_PORT]\n",
                "# [LOCAL_PORT]: Listen on local port 9090 on your local machine (e.g., your laptop)\n",
                "# [REMOTE_HOST]: Tunnel that traffic through the SSH connection. This refers to \"localhost\" from the perspective of the remote server (in this case, the FABRIC node you're SSH-ing into).\n",
                "# [REMOTE_PORT]: Forward it to port 30906 on localhost from the perspective of the remote FABRIC node. This is the port on the remote machine (the FABRIC node) that you're connecting to — in this case, the NodePort exposed by Prometheus\n",
                "```\n",
                "You can do the same for any other service, such as Grafana. Below are the same commands for Grafana, but then without the above explanation, since it is exactly the same:\n",
                "```sh\n",
                "# From SSH in the k8s-control-plane node:\n",
                "kubectl get svc prometheus-grafana -n monitoring\n",
                "# Example output:\n",
                "ubuntu@k8s-control-plane:~$ kubectl get svc prometheus-grafana -n monitoring\n",
                "NAME                 TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE\n",
                "prometheus-grafana   NodePort   10.108.65.229   <none>        80:31092/TCP   15h\n",
                "# Command with example output for SSH tunnel:\n",
                "ssh -i ~/.ssh/slice_key -F ssh_config -L 3000:localhost:31092 ubuntu@2001:610:2d0:fabc:f816:3eff:fea4:5b34\n",
                "# Access it at (on your local machine): http://localhost:3000/\n",
                "# Login with username: admin\n",
                "# Get the password:\n",
                "kubectl get secret -n monitoring prometheus-grafana -o jsonpath=\"{.data.admin-password}\" | base64 --decode ; echo\n",
                "```\n",
                "For how to add the Kepler dashboard, see energy-efficiency/docs/getting-started/3_SetupMonitoring.md > Preparing Kepler (energy measurements)."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
